{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torchtext as tt\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchtext.vocab import Vocab\n\nimport typing\nfrom typing import List\nfrom collections import Counter\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/hinglish-english/Langauge_Data.csv')\ndf_valid = pd.read_csv('../input/hinglish-english/testing_data_.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid.info()\ndf_valid.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid = df_valid.rename(columns = {'word':'Word', 'label':'Langauge'})\ndf_valid = df_valid.drop(['Unnamed: 0'], axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(word):\n    return [c for c in word]\n\ndef cleaner(word):\n    word = str(word)\n    word = word.lower()\n    \n    word = clear_punk(word)\n    word = word.strip()\n    return word\n\ndef build_vocab(data, min_fre, tokenizer):\n    counter = Counter()\n    for word in data:\n        word = cleaner(word)\n        counter.update(tokenizer((word)))\n    return Vocab(counter,min_freq=min_fre , specials=( '<unk>','<pad>', '<sos>', '<eos>'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = build_vocab(df_train['Word'], min_fre=1, tokenizer = tokenize)\nvocab.stoi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Word'] = df_train['Word'].apply(lambda x: str(x))\n(df_train['Word'].apply(len)).max()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Modelling**","metadata":{}},{"cell_type":"code","source":"class LangData(Dataset):\n    def __init__(self, df, vocab, maxlen=32):\n        self.df = df\n        self.vocab = vocab\n        self.maxlen = maxlen\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        df = self.df\n        word = str(df.loc[idx, 'Word'])\n        word = cleaner(word)\n        tokens = tokenize(word)\n        tokens = [vocab.stoi[token] for token in tokens]\n        tokens += [1] * (self.maxlen - len(tokens))\n        \n        label = int(df.loc[idx, 'Langauge'])\n        \n        return np.array(tokens), label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LangDetectNet(nn.Module):\n    def __init__(self, vocab_size:int, embedding_dim:int, n_layers:int, hidden_dim: int):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.LSTM = nn.LSTM(input_size = embedding_dim,\n                            hidden_size = hidden_dim, \n                            num_layers = n_layers, \n                            dropout=0, \n                            bidirectional=True,\n                            batch_first = True)\n        self.fc1 = nn.Linear(2*hidden_dim, 5)\n        self.fc2 = nn.Linear(5,1)\n        self.relu = nn.ReLU()\n        \n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_x: torch.TensorType):\n        # shape of x: [seq_len, batch_size]\n        x = self.embedding(input_x)\n        #shape of x: [seq_len, batch_size, embedding_dim]\n        outp, (hidden, cell) = self.LSTM(x)\n\n        # shape of outp: [seq_len, batch_size, 2*hidden_dim]\n        hidden_last = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n        x = self.relu(self.fc1(hidden_last))\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nProgress_Bar = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion, device):\n    \n    epoch_loss = 0\n    model.train()\n    bar = tqdm(iterator) if Progress_Bar else iterator\n    \n    for (x, y) in bar:\n        x = torch.LongTensor(x)\n        \n        y = y.float()\n        x = x.to(device)\n        y = y.to(device)\n        optimizer.zero_grad()\n        y_pred = model(x)\n        y_pred = torch.reshape(y_pred, (-1,))\n        loss = criterion(y_pred, y)\n        loss.backward()\n        optimizer.step()\n        loss_np = loss.detach().cpu().numpy()\n        epoch_loss += loss_np\n        if Progress_Bar:\n            bar.set_description('Training loss: %.5f' % (loss_np))\n        \n    return epoch_loss/len(iterator)\n\ndef evaluate(model, iterator, criterion, device):\n    \n    epoch_loss = 0\n    preds = []\n    preds = np.array(preds)\n    targets = []\n    targets = np.array(targets)\n    model.eval()\n    bar = tqdm(iterator) if Progress_Bar else iterator\n    \n    with torch.no_grad():\n        \n        for (x, y) in bar:\n            x = torch.LongTensor(x)\n            y = y.float()\n            x = x.to(device)\n            y = y.to(device)\n            y_pred = model(x)\n            y_pred = torch.reshape(y_pred, (-1,))\n            loss = criterion(y_pred, y)\n            loss_np = loss.detach().cpu().numpy()\n            epoch_loss += loss_np\n            \n            y_pred = y_pred.detach().cpu().numpy()\n            y_pred = [1 if pred>0.5 else 0 for pred in y_pred]\n            preds = np.append(preds, y_pred)\n            targets = np.append(targets, y.detach().cpu().numpy())\n#             preds = preds.reshape(-1)\n#             targets = targets.reshape(-1)\n            \n            if Progress_Bar:\n                bar.set_description('Validation loss: %.5f' % (loss_np))\n            \n            \n     \n    return epoch_loss/len(iterator), accuracy_score(preds, targets)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\ndef fit_model(model, model_name, train_iterator, valid_iterator, optimizer, loss_criterion, device, epochs):\n    best_valid_loss = float('inf')\n    \n    train_losses = []\n    valid_losses = []\n    valid_metric_scores = []\n    \n    for epoch in range(epochs):\n    \n        start_time = time.time()\n    \n        train_loss = train(model, train_iterator, optimizer, loss_criterion, device)\n        valid_loss, valid_metric_score = evaluate(model, valid_iterator, loss_criterion, device)\n        \n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n        valid_metric_scores.append(valid_metric_score)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), f'{model_name}.pt')\n    \n        end_time = time.time()\n\n        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n        print(f'Epoch: {epoch+1: 2} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n        print(f'Train Loss: {train_loss:.3f}')\n        print(f'Val. Loss: {valid_loss:.3f} |  Val. Metric Score: {valid_metric_score:.3f}')\n        \n    return train_losses, valid_losses, valid_metric_scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = LangData(df_train, vocab)\ntrain_loader = DataLoader(train_data, shuffle = True, batch_size = 64, num_workers = 2)\n\nvalid_data = LangData(df_valid, vocab)\nvalid_loader = DataLoader(valid_data, shuffle = True, batch_size = 64, num_workers = 2)\n\nmodel = LangDetectNet(vocab_size = len(vocab.stoi),\n                   embedding_dim = 3,\n                    hidden_dim = 5,\n                   n_layers = 2 ).to(device)\n\nloss_criterion = nn.BCELoss()\n\nopt = optim.Adam(model.parameters(), lr = 1e-2, betas=(0.9,0.999))\nfit_model(model, 'LangaugeDetection',  train_loader, valid_loader, opt, loss_criterion, device, epochs = 7)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}